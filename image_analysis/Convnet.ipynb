{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named keras",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e92526705445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNativeTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPublicEncodedTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPrivateEncodedTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReluExact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReveal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrossEntropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftmaxStable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m                                                                                   \u001b[0mAveragePooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConvAveragePooling2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named keras"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from pond.tensor import NativeTensor, PublicEncodedTensor, PrivateEncodedTensor\n",
    "from pond.nn import Dense, ReluExact, Relu, Reveal, CrossEntropy, SoftmaxStable, Sequential, DataLoader, Conv2D, \\\n",
    "                                                                                  AveragePooling2D, Flatten, ConvAveragePooling2D\n",
    "from keras.utils import to_categorical\n",
    "np.random.seed(42)\n",
    "\n",
    "# set errors error behaviour for overflow/underflow\n",
    "_ = np.seterr(over='raise')\n",
    "_ = np.seterr(under='raise')\n",
    "_ = np.seterr(invalid='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train[:,np.newaxis,:,:] / 255.0\n",
    "x_test = x_test[:,np.newaxis,:,:] / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "batch_size = 128\n",
    "input_shape = [batch_size] + list(x_train.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define 2 convnets\n",
    "We define 2 convnets, one with a single convlayer and one with 2 convlayers. Both can be used, but a single layer convnet is already quite slow. You can also use the ConvAveragePooling2D layer instead of the separated Conv2D and AveragePooling2D layers, to reduce comunication. The Relu layer we use here is an approximation of the regular ReLU, which can be called by replacing ReLU(order=3) with ReluExact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convnet_shallow = Sequential([\n",
    "    Conv2D((3, 3, 1, 32), strides=1, padding=1, filter_init=lambda shp: np.random.normal(scale=0.1, size=shp)),\n",
    "    AveragePooling2D(pool_size=(2, 2)),\n",
    "    Relu(order=3),\n",
    "    Flatten(),\n",
    "    Dense(10, 6272),\n",
    "    Reveal(),\n",
    "    SoftmaxStable()\n",
    "])\n",
    "\n",
    "\n",
    "convnet_deep = Sequential([\n",
    "    Conv2D((3, 3, 1, 32), strides=1, padding=1, filter_init=lambda shp: np.random.uniform(low=-0.14, high=0.14, size=shp)),\n",
    "    AveragePooling2D(pool_size=(2, 2)),\n",
    "    Relu(),\n",
    "    Conv2D((3, 3, 32, 32), strides=1, padding=1, filter_init=lambda shp: np.random.uniform(low=-0.1, high=0.1, size=shp)),\n",
    "    AveragePooling2D(pool_size=(2, 2)),\n",
    "    Relu(),\n",
    "    Flatten(),\n",
    "    Dense(10, 1568),\n",
    "    Reveal(),\n",
    "    SoftmaxStable()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-14 14:56:34.498623 Epoch 1/1\n",
      "60032/60032 [=============================>.] - ETA: 0:00:01 - train_loss: 0.47501 - train_acc 0.93750 - val_loss 0.56720 - val_acc 0.86810\n"
     ]
    }
   ],
   "source": [
    "tensortype = NativeTensor\n",
    "convnet_shallow.initialize(initializer=tensortype, input_shape=input_shape)\n",
    "convnet_shallow.fit(\n",
    "    x_train=DataLoader(x_train, wrapper=tensortype),\n",
    "    y_train=DataLoader(y_train, wrapper=tensortype),\n",
    "    x_valid=DataLoader(x_test, wrapper=tensortype),\n",
    "    y_valid=DataLoader(y_test, wrapper=tensortype),\n",
    "    loss=CrossEntropy(),\n",
    "    epochs=1,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    learning_rate=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PublicEncodedTensor\n",
    "Train the same network on PublicEncodedTensor, this network **does not** use SPDZ, but works on the 128 bit integer encoding of real numbers which is neccesary for SPDZ, but slows down 100x. (however there are fixes for this) Therefore, we only train on the first two batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-14 15:03:13.388719 Epoch 1/1\n",
      "256/256 [===============>...............] - ETA: 0:01:22 - train_loss: 2.31062 - train_acc 0.09375 - val_loss 2.30344 - val_acc 0.12500\n"
     ]
    }
   ],
   "source": [
    "# train on small set\n",
    "x_train = x_train[:256]\n",
    "y_train = y_train[:256]\n",
    "x_test = x_test[:256]\n",
    "y_test = y_test[:256]\n",
    "\n",
    "tensortype = PublicEncodedTensor\n",
    "convnet_shallow.initialize(initializer=tensortype, input_shape=input_shape)\n",
    "convnet_shallow.fit(\n",
    "    x_train=DataLoader(x_train, wrapper=tensortype),\n",
    "    y_train=DataLoader(y_train, wrapper=tensortype),\n",
    "    x_valid=DataLoader(x_test, wrapper=tensortype),\n",
    "    y_valid=DataLoader(y_test, wrapper=tensortype),\n",
    "    loss=CrossEntropy(),\n",
    "    epochs=1,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    learning_rate=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know, the progressbar has a bug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrivateEncodedTensor\n",
    "Train the same network on PublicEncodedTensor, this network **does** use SPDZ, and is therefore even slower. Here, we also train on the first two batches. However, if you have time you can run the network for a full epoch and reach ~86% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-14 15:04:42.588405 Epoch 1/1\n",
      "256/256 [===============>...............] - ETA: 0:16:23 - train_loss: 2.30855 - train_acc 0.12500 - val_loss 2.29269 - val_acc 0.13672\n"
     ]
    }
   ],
   "source": [
    "# train on small set\n",
    "x_train = x_train[:256]\n",
    "y_train = y_train[:256]\n",
    "x_test = x_test[:256]\n",
    "y_test = y_test[:256]\n",
    "\n",
    "tensortype = PrivateEncodedTensor\n",
    "convnet_shallow.initialize(initializer=tensortype, input_shape=input_shape)\n",
    "convnet_shallow.fit(\n",
    "    x_train=DataLoader(x_train, wrapper=tensortype),\n",
    "    y_train=DataLoader(y_train, wrapper=tensortype),\n",
    "    x_valid=DataLoader(x_test, wrapper=tensortype),\n",
    "    y_valid=DataLoader(y_test, wrapper=tensortype),\n",
    "    loss=CrossEntropy(),\n",
    "    epochs=1,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    learning_rate=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple network is already very slow. Therefore, [this](http://mortendahl.github.io/2018/03/01/secure-computation-as-dataflow-programs/#basics) is not a bad idea."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
